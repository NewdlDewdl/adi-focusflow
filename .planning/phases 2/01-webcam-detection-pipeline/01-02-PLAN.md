---
phase: 01-webcam-detection-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/hooks/useHumanDetection.ts
  - src/components/detection/WebcamView.tsx
  - src/components/detection/DetectionProvider.tsx
  - src/app/page.tsx
autonomous: true

must_haves:
  truths:
    - "User sees their face detected in real-time with visible gaze direction and head pose data overlaid on the video feed"
    - "Detection runs at a throttled 5 Hz rate (200ms intervals), not at max framerate"
    - "Human.js models load with a visible loading indicator before first detection"
  artifacts:
    - path: "src/hooks/useHumanDetection.ts"
      provides: "Detection loop lifecycle with throttled inference and temporal smoothing"
      exports: ["useHumanDetection"]
    - path: "src/components/detection/WebcamView.tsx"
      provides: "Video element with canvas overlay for detection visualization"
      min_lines: 50
  key_links:
    - from: "src/hooks/useHumanDetection.ts"
      to: "src/lib/human-config.ts"
      via: "import humanConfig for Human.js initialization"
      pattern: "import.*humanConfig.*from.*human-config"
    - from: "src/hooks/useHumanDetection.ts"
      to: "@vladmandic/human"
      via: "Human class instantiation and detect() loop"
      pattern: "new Human|human\\.detect"
    - from: "src/components/detection/WebcamView.tsx"
      to: "src/hooks/useHumanDetection.ts"
      via: "useHumanDetection hook consuming videoRef"
      pattern: "useHumanDetection"
    - from: "src/components/detection/DetectionProvider.tsx"
      to: "src/components/detection/WebcamView.tsx"
      via: "renders WebcamView when permission granted"
      pattern: "WebcamView"
---

<objective>
Build the core detection loop and webcam visualization with face detection, gaze tracking, and head pose overlay.

Purpose: This is the heart of Phase 1 -- the actual Human.js detection pipeline. The detection hook manages model loading, warmup, throttled inference at 5 Hz, and temporal smoothing. The webcam view renders the video feed with a canvas overlay showing bounding boxes, gaze direction, and head pose data. This plan produces the detection data that Phase 2 (Focus Scoring) will consume.

Output: A working webcam detection pipeline showing real-time face detection with gaze and head pose data overlaid on the video feed. Models load with a visible indicator. Detection runs at 200ms intervals (5 Hz), not at max framerate.
</objective>

<execution_context>
@/Users/adiga/.claude/get-shit-done/workflows/execute-plan.md
@/Users/adiga/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-webcam-detection-pipeline/01-RESEARCH.md
@.planning/phases/01-webcam-detection-pipeline/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create the Human.js detection loop hook with model loading and temporal smoothing</name>
  <files>
    src/hooks/useHumanDetection.ts
  </files>
  <action>
    Create `src/hooks/useHumanDetection.ts` as a `'use client'` hook that manages the entire Human.js lifecycle:

    **Initialization phase (runs once):**
    - Import Human from `@vladmandic/human` and humanConfig from `@/lib/human-config`
    - Create a single Human instance via `new Human(humanConfig)`
    - Call `await human.load()` to preload models (15-25 MB from CDN, cached in IndexedDB after first load)
    - Call `await human.warmup()` to initialize the GPU pipeline (reduces first-frame latency)
    - Track loading state: `{ isLoading: boolean, loadingMessage: string, isReady: boolean }`
    - Use `human.events` EventTarget to track load/warmup progress if available, otherwise use step-based messages ("Loading models...", "Warming up GPU...", "Ready")

    **Detection loop (runs after initialization + when videoRef has active stream):**
    - Use `setTimeout`-based loop at 200ms intervals (PERF-04: 5 Hz detection rate). NOT requestAnimationFrame (which runs at 60fps and would queue up inference calls faster than they complete)
    - Each iteration: `const res = await human.detect(videoRef.current)`
    - Apply temporal smoothing: `const interpolated = human.next(res)` -- this smooths results between detection frames so UI does not jump
    - Store result in state for UI consumption
    - Guard against concurrent detect() calls: use a `runningRef` boolean to prevent overlap if a detect() call takes longer than 200ms
    - Calculate FPS from detection timing: `1000 / (now - lastDetectTime)`
    - Read tensor count: `human.tf.memory().numTensors`

    **Cleanup:**
    - Set runningRef to false to stop the detection loop
    - Set humanRef to null (Human.js does not have a dispose() method; null allows GC)

    **Hook signature:**
    ```typescript
    function useHumanDetection(videoRef: React.RefObject<HTMLVideoElement | null>): {
      result: DetectionResult | null;
      fps: number;
      tensorCount: number;
      isLoading: boolean;
      loadingMessage: string;
      isReady: boolean;
      humanRef: React.RefObject<Human | null>;
    }
    ```

    Use the DetectionResult type from `@/lib/detection-types.ts`. The result should be shaped from `human.detect()` output, mapping face[0] to the FaceDetectionData interface.

    CRITICAL: Do NOT set `face.detector.return: true` anywhere -- this outputs raw tensors that must be manually disposed and will cause memory leaks.

    CRITICAL: Do NOT use `setInterval` -- use recursive `setTimeout` to ensure the next detection only starts after the previous one completes. This prevents queueing.
  </action>
  <verify>
    1. `npx tsc --noEmit` passes with no type errors
    2. Hook can be imported without errors: add a temporary console.log in DetectionProvider to verify
    3. Verify the hook does NOT import requestAnimationFrame or setInterval for the detection loop
    4. Verify humanConfig uses 'humangl' backend, not 'webgl'
  </verify>
  <done>
    Detection hook manages full Human.js lifecycle: load -> warmup -> throttled detect loop at 200ms intervals -> temporal smoothing via human.next(). FPS and tensor count are tracked. Loading state is exposed for UI. No concurrent detect() calls possible. Cleanup stops loop on unmount.
  </done>
</task>

<task type="auto">
  <name>Task 2: Build webcam view with canvas overlay showing face detection, gaze, and head pose</name>
  <files>
    src/components/detection/WebcamView.tsx
    src/components/detection/DetectionProvider.tsx
    src/app/page.tsx
  </files>
  <action>
    Create `src/components/detection/WebcamView.tsx` (client component):
    - Accepts videoRef from useWebcamPermission and detection result from useHumanDetection
    - Renders a `<video>` element with attributes: autoPlay, playsInline, muted (required for browser compatibility)
    - Renders a `<canvas>` element positioned absolutely over the video for drawing detection overlays
    - Video and canvas should be sized consistently (640x480 or responsive with aspect ratio maintained)

    **Detection overlay drawing (on each new detection result):**
    Use `human.draw.all(canvas, result)` for the primary overlay (bounding boxes, face mesh wireframe, gaze direction arrows). Human.js draw API handles canvas sizing and coordinate mapping.

    Additionally, draw a custom text overlay showing key metrics:
    - Head pose: "Yaw: {deg}  Pitch: {deg}  Roll: {deg}" (convert radians to degrees)
    - Gaze: "Gaze: {bearing_deg} ({strength_percent}%)"
    - Face confidence: "Face: {score_percent}%"
    - Position this text in the top-left corner of the canvas with a semi-transparent background for readability

    Use `useEffect` or `useCallback` to redraw the canvas whenever the detection result changes. Access the Human instance from humanRef (passed from useHumanDetection) to call `human.draw.all()`.

    **Loading state:**
    When isLoading is true, show a loading overlay on top of the video area:
    - Centered spinner or progress indicator
    - Show loadingMessage ("Loading models...", "Warming up GPU...", etc.)
    - Semi-transparent backdrop so user can see camera preview underneath

    Update `src/components/detection/DetectionProvider.tsx`:
    - When permission is granted, initialize useHumanDetection with the videoRef from useWebcamPermission
    - Render WebcamView with all necessary props
    - Show PrivacyIndicator
    - Show basic detection status text (FPS, face detected yes/no)

    Update `src/app/page.tsx` if needed to ensure the dynamic import and layout work correctly. The page should have a clean layout with:
    - App title "FocusFlow" at the top
    - The detection view centered
    - Clean, dark or neutral background that works well with webcam video
  </action>
  <verify>
    1. `npm run dev` -- visit http://localhost:3000
    2. Grant camera permission -- see webcam video feed
    3. Loading indicator appears while models load (~6 seconds for HumanGL warmup)
    4. After loading, face is detected with bounding box and mesh overlay visible
    5. Head pose angles (yaw, pitch, roll) displayed as text overlay on the canvas
    6. Gaze direction data (bearing, strength) displayed on the canvas
    7. Turn head left/right -- yaw angle changes visibly
    8. Look up/down -- pitch angle changes visibly
    9. Look away from screen -- gaze bearing changes visibly
    10. `npx tsc --noEmit` passes
  </verify>
  <done>
    Webcam view shows real-time face detection with Human.js draw API overlay (bounding box, face mesh, gaze arrows). Head pose (yaw/pitch/roll) and gaze (bearing/strength) are displayed as text overlay. Loading indicator shows during model initialization (~6 seconds). Detection runs at 5 Hz (200ms intervals). Video has autoPlay, playsInline, muted attributes for browser compatibility.
  </done>
</task>

</tasks>

<verification>
1. Face detection works in real-time with visible bounding box and mesh overlay
2. Head pose data (yaw, pitch, roll) updates as user turns their head
3. Gaze data (bearing, strength) updates as user looks in different directions
4. Model loading shows a progress/loading indicator (not a blank screen for 6 seconds)
5. Detection runs at ~5 FPS (200ms intervals), not at 60 FPS
6. FPS counter is visible showing actual detection rate
7. Turning away from camera shows yaw change; looking down shows pitch change
8. `npx tsc --noEmit` produces zero errors
</verification>

<success_criteria>
- Real-time face detection with bounding box, mesh, and gaze overlay on webcam feed
- Head pose (yaw/pitch/roll in degrees) displayed on canvas
- Gaze direction (bearing/strength) displayed on canvas
- Loading indicator during Human.js model load and warmup
- Detection loop running at 5 Hz (200ms throttle via recursive setTimeout)
- Temporal smoothing via human.next() applied to detection results
- FPS and tensor count tracked and exposed by the detection hook
- All TypeScript types clean (zero tsc errors)
</success_criteria>

<output>
After completion, create `.planning/phases/01-webcam-detection-pipeline/01-02-SUMMARY.md`
</output>
